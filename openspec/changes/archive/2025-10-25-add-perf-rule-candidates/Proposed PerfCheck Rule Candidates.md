# Proposed PerfCheck Rule Candidates
## Candidate Rules
| candidate_rule_id              | language | problem_pattern                                                                                                                           | why_it_matters                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | detection_approach                                                                                                                                                                                                                                                                                                                                                          | references                                                                                                                                                                                                                                                                                                   |
|--------------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **perf_avoid_linked_list**     | both     | Using linked-list data structures (Go’s container/list or Rust’s LinkedList) for general sequences instead of array-backed containers.    | Linked lists impose heavy pointer chasing and poor cache locality, making operations much slower than using slices or vectors in most cases. In benchmarks, a simple append loop on a Go slice ran ~20× faster per operation than using a list.List.                                                                                                                                                                                                                                         | Flag any instantiation or import of linked-list types (container/list in Go, std::collections::LinkedList in Rust). Suggest using dynamic arrays (slices/Vec) unless random insert/delete in middle dominates workload.                                                                                                                                                     | Clippy pedantic lint notes LinkedList wastes memory and is “all-around slow”; Stack Overflow benchmarks show extreme overhead of list.List vs slice.                                                                                                                                                         |
| **perf_large_enum_variant**    | rust     | Defining an enum where one variant holds a much larger data structure than others.                                                        | Rust enum size is determined by its largest variant. A single bulky variant (e.g. an 8000-element array) bloats every enum instance’s size, wasting memory and hurting cache efficiency. This can significantly increase stack usage and copy costs for the enum.                                                                                                                                                                                                                            | Statically compare enum variant types: if one variant’s estimated size exceeds another by a large factor (e.g. >5–10×), flag it. PerfCheck (with compiler type info) could suggest boxing the large payload to reduce enum size.                                                                                                                                            | Clippy’s large_enum_variant warns that one big variant “penalizes the memory layout” of the enum.                                                                                                                                                                                                            |
| **perf_unnecessary_arc**       | rust     | Using Arc<T> for data that is never shared across threads (especially if T is not Send + Sync).                                           | Arc<T> (atomic reference counting) incurs extra cost on every clone and drop due to atomic operations, whereas Rc<T> is cheaper for single-thread cases. If the contained type isn’t thread-safe, Arc’s thread-safety guarantees add no benefit – it’s pure overhead.                                                                                                                                                                                                                        | Detect std::sync::Arc usage where the type T is not Send/Sync (e.g. contains RefCell or other !Send data). Such cases can be flagged to use Rc. (Requires type trait analysis or a heuristic that no multi-threading is observed in the code.)                                                                                                                              | Rust Clippy notes that Arc uses atomic ops and should be replaced with Rc if T isn’t Send+Sync. This avoids unnecessary atomic ref counting.                                                                                                                                                                 |
| **perf_atomic_for_small_lock** | both     | Using a full mutex lock to guard a single primitive value (like a boolean flag or counter) with no other critical section logic.          | Locking a Mutex for a simple value is overkill – it introduces syscall or scheduling overhead and contention management. An atomic type can often perform the same operation with far less overhead. For example, AtomicBool or AtomicUsize updates avoid the kernel trap of a mutex and scale better under contention.                                                                                                                                                                      | Look for uses of mutex types (sync.Mutex in Go, std::sync::Mutex in Rust) whose protected data is a plain integer, bool, pointer, or similar. If no complex invariant requires mutual exclusion, suggest using an atomic. (Exclude cases paired with condition variables or multi-step critical sections.)                                                                  | Clippy’s mutex_atomic/mutex_integer lints note that using a mutex for a plain bool or integer is “shooting flies with cannons” and that atomics are leaner and faster.                                                                                                                                       |
| **perf_no_defer_in_loop**      | go       | Deferring function calls or resource releases inside a loop.                                                                              | A defer in a loop delays each deferred call until the function returns, causing accumulation of deferred calls. This not only postpones resource cleanup (risking e.g. file descriptor exhaustion) but also forces Go to heap-allocate the defer records since their count isn’t known at compile time. The result is significant memory and CPU overhead for hot loops.                                                                                                                     | Scan for the defer keyword inside loop bodies. If found, warn that the defer will not execute until after the loop and suggest moving the operation out of the loop (or manually calling the function at loop end) for better performance and timely cleanup.                                                                                                               | Go issue reports confirm each loop-deferred call incurs heap allocation and overhead. Best practice guides caution against defers in tight loops due to this accumulated cost.                                                                                                                               |
| **perf_avoid_rune_conversion** | go       | Converting a string to a slice of runes ([]rune(str)) in order to iterate over its characters.                                            | Converting to []rune allocates a new slice and copies the entire string into memory. If the goal is just to iterate over characters, a for range on the string yields the same runes without allocation. Avoiding the temporary slice can significantly reduce memory churn and improve speed when processing strings in hot code.                                                                                                                                                           | Trigger on patterns where a string is explicitly converted to []rune and immediately ranged over or processed element-wise. A simple AST match for for ... range []rune(var) is a clear signal. Suggest iterating over the string directly.                                                                                                                                 | Staticcheck flag SA6003 notes that ranging over a string is more efficient than []rune(s) because it “will be faster and avoid unnecessary memory allocations”.                                                                                                                                              |
| **perf_needless_collect**      | rust     | Collecting an iterator into an intermediate vector or collection when it isn’t necessary for the computation.                             | Unneeded collection causes needless heap allocation and copying. For example, doing iter.collect::<Vec<_>>().len() allocates a vector just to count elements, whereas iter.count() does the same with no allocation. In performance-sensitive code, avoiding these temporary allocations reduces GC/allocator pressure and improves cache usage.                                                                                                                                             | Use AST or MIR analysis to find cases where .collect() is called but the collected container is only used for a simple size, iteration, or immediately thrown away. PerfCheck can then suggest using iterator methods (count, iteration combinators, etc.) that don’t require building a full collection.                                                                   | Clippy’s needless_collect lint explains that collecting just to perform a computation (like length) is wasteful. Removing the intermediate allocation can streamline the operation.                                                                                                                          |
| **perf_use_buffered_io**       | go       | Performing many small read/write operations to files or network streams without buffering.                                                | Unbuffered I/O does a system call for each tiny operation, which is very slow. For instance, writing one byte at a time to a file will invoke a write syscall per byte. Using bufio.Writer or bufio.Reader to batch data can drastically improve throughput by reducing syscall frequency. This yields order-of-magnitude speedups for I/O heavy loops.                                                                                                                                      | Heuristic: detect loops or frequent calls where io.Writer.Write or io.Reader.Read (or fmt.Fprint, etc.) are invoked with small payloads (e.g. byte-by-byte or line-by-line writes). If the code isn’t already using bufio.NewWriter/NewReader, suggest introducing buffering. Also flag creating an os.File or net.Conn and writing small chunks to it repeatedly.          | Go documentation and guides emphasize that “many small writes can hurt performance” and should be buffered. Empirical tests show buffered I/O vastly outperforming unbuffered for large numbers of small operations.                                                                                         |
| **perf_prefer_stack_alloc**    | both     | Unnecessarily allocating small objects on the heap (e.g. using pointers or Box for small structs or values that could live on the stack). | For small data sizes, stack allocation and value semantics are typically faster than heap allocation. Accessing stack memory is very fast, and copying a few bytes is cheap. By contrast, heap allocation incurs malloc/free overhead and pointer indirection. For example, in Go, passing a small struct by value was ~4× faster and zero-allocation compared to passing a pointer (which caused an 80-byte alloc). Overusing heap where not needed increases GC pressure and cache misses. | Look for function signatures or struct fields that use pointers to basic structs or values where the pointed-to type is below a certain size (e.g. a couple of words). In Rust, flag Box<T> usage for types that implement Copy or are <= some byte size and could be stored or passed directly. These patterns suggest a possible refactor to avoid the extra indirection. | Performance analyses note that small structs often perform better on the stack. A Boot.dev benchmark showed passing a 80-byte struct by value beat passing by pointer (no allocations vs 1 allocation per loop). Clippy also flags gratuitous indirection (e.g. Rc<Box<T>> or Box<&T>) as unneeded overhead. |
## Research Notes
The above candidates address several gaps where idiomatic Go and Rust code can harbor hidden performance costs. Notably, many involve avoiding unnecessary allocations or indirections (e.g. using contiguous arrays instead of pointer-heavy lists, eliminating needless temporary objects, or favoring stack vs heap memory). These are generally **high-impact** in hot paths, as supported by recent benchmarks and linter documentation.
During research, it became clear that some patterns (like **large enum variants** or **Arc vs Rc usage**) would require **language-specific analysis**. For Rust, PerfCheck might need **compiler type information or MIR** to reliably detect things like variant sizes or trait bounds (e.g. to know a type is !Send). Such rules might be feature-gated or run only when advanced analysis is available. In contrast, many Go patterns (e.g. defers in loops, unbuffered I/O, string-to-rune conversion) are detectable via simpler AST patterns and could be “quick wins” for implementation.
One emerging trend is encouraging more **cache-friendly code**: hence the suggestion to flag linked-list usage and large stack frames (from huge enum variants or unchecked large local slices). Another trend is reducing **GC pressure** in Go – rules about preallocating, pooling, and avoiding tiny allocs align with community advice (our new rules about buffered I/O, avoiding string concat (already covered), and minimizing allocations support this). We verified that each proposed pattern is mentioned in credible sources (Clippy lints, staticcheck, Go team writings, or performance postmortems), indicating real-world impact.
**False positives vs. false negatives** were weighed. For example, using a Mutex instead of an atomic might be intentional for more complex synchronization (not just simple value protection); our **perf_atomic_for_small_lock** rule should perhaps be allow-by-default or provide clear justification text, to be used judiciously. Similarly, the linked-list rule might need an allowance if the code truly benefits from pointer-splicing (though such cases are rare). These could be handled via configuration or lint levels (e.g. Clippy marks linkedlist as pedantic due to rare valid uses).
**Prerequisites**: Many proposed detections can work with syntax alone (e.g. spotting defer in loops or for range []rune). Others benefit from type/context info: - **Large enum variant**: needs computing variant sizes (likely from rustc’s layout info). - **Unnecessary Arc**: needs trait knowledge (T: Send+Sync?) to be certain, or else a heuristic (flag Arc of clearly non-thread-safe types like RefCell). - **Atomic vs Mutex**: requires identifying the inner type of the lock; in Rust, this is via generics, in Go via field analysis. PerfCheck might integrate with type-checkers or limit to known patterns (Mutex of int, bool, etc.). - **Prefer stack alloc**: determining “small” could use a rough threshold of fields/count of primitives if exact size isn’t easily got. We might start conservative (flag obvious cases like pointer to an int or bool, Box<u32>).
We should highlight that rules like **needless_collect** or **prefer buffered I/O** are relatively straightforward to implement and test, as they focus on common patterns with clear alternatives. In contrast, **large_enum_variant** or **large**** ****async**** ****future** (an async-related variant not listed above, but noted by Clippy) would be more advanced, requiring deeper integration with compiler internals.
Finally, none of the above duplicate existing PerfCheck rules. They instead extend coverage into areas identified by other linters and performance guides. By cross-referencing Clippy and Staticcheck, we ensured these suggestions are grounded in proven practice. Future follow-up could include monitoring new Rust RFCs (for example, improvements in defer or new atomic APIs) and Go proposals (like potential optimizer changes around small defers or escape analysis) that might affect the relevance of these rules. Overall, these candidates balance **practical impact** with **feasibility of static detection**, pushing PerfCheck’s default performance checks into more nuanced territory.

Clippy Lints

go - Differences between slice and container/list - Stack Overflow

avoid defers in loops · Issue #137605 · cockroachdb/cockroach · GitHub

Checks | Staticcheck

Introduction to bufio package in Golang | by Michał Łowicki | golangspec | Medium

Are Pointers in Go Faster Than Values? | Boot.dev
